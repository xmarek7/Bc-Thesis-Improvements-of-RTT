%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  digital,     %% The `digital` option enables the default options for the
               %% digital version of a document. Replace with `printed`
               %% to enable the default options for the printed version
               %% of a document.
%%  color,       %% Uncomment these lines (by removing the %% at the
%%               %% beginning) to use color in the printed version of your
%%               %% document
  oneside,     %% The `oneside` option enables one-sided typesetting,
               %% which is preferred if you are only going to submit a
               %% digital version of your thesis. Replace with `twoside`
               %% for double-sided typesetting if you are planning to
               %% also print your thesis. For double-sided typesetting,
               %% use at least 120 g/m² paper to prevent show-through.
  nosansbold,  %% The `nosansbold` option prevents the use of the
               %% sans-serif type face for bold text. Replace with
               %% `sansbold` to use sans-serif type face for bold text.
  nocolorbold, %% The `nocolorbold` option disables the usage of the
               %% blue color for bold text, instead using black. Replace
               %% with `colorbold` to use blue for bold text.
  nolof,         %% The `lof` option prints the List of Figures. Replace
               %% with `nolof` to hide the List of Figures.
  nolot,         %% The `lot` option prints the List of Tables. Replace
               %% with `nolot` to hide the List of Tables.
]{fithesis4}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.
\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  english, german, russian, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{russian} ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%% For non-Latin scripts, it may be necessary to load additional
%% fonts:
\usepackage{paratype}
\usepackage{todonotes}
\setuptodonotes{inline}
\def\textrussian#1{{\usefont{T2A}{PTSerif-TLF}{m}{rm}#1}}
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{
    date        = \the\year/\the\month/\the\day,
    university  = mu,
    faculty     = fi,
    type        = bc,
    department  = Department of Computer Systems and Communications,
    author      = Tomáš Marek,
    gender      = f,
    advisor     = {Ing. Milan Brož, Ph.D.},
    title       = {Improvements of the Randomness Testing Toolkit},
    TeXtitle    = {Improvements of the Randomness Testing Toolkit},
    keywords    = {keyword1, keyword2, ...},
    TeXkeywords = {keyword1, keyword2, \ldots},
    abstract    = {%
      This is the abstract of my thesis, which can

      span multiple paragraphs.
    },
    thanks      = {%
      These are the acknowledgements for my thesis, which can

      span multiple paragraphs.
    },
    bib         = bibliography.bib,
    %% Remove the following line to use the JVS 2018 faculty logo.
    facultyLogo = fithesis-fi,
}
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist} %% Compact list environments
\usepackage{amsmath}  %% Mathematics
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}      %% Hyperlinks
\usepackage{markdown} %% Lightweight markup
\usepackage{listings} %% Source code highlighting
\lstset{
  basicstyle      = \ttfamily,
  identifierstyle = \color{black},
  keywordstyle    = \color{blue},
  keywordstyle    = {[2]\color{cyan}},
  keywordstyle    = {[3]\color{olive}},
  stringstyle     = \color{teal},
  commentstyle    = \itshape\color{magenta},
  breaklines      = true,
}
\usepackage{floatrow} %% Putting captions above tables
\floatsetup[table]{capposition=top}
\usepackage[babel]{csquotes} %% Context-sensitive quotation marks
\begin{document}
%% The \chapter* command can be used to produce unnumbered chapters:
\chapter*{Introduction}
%% Unlike \chapter, \chapter* does not update the headings and does not
%% enter the chapter to the table of contents. I we want correct
%% headings and a table of contents entry, we must add them manually:
\markright{\textsc{Introduction}}
\addcontentsline{toc}{chapter}{Introduction}
\todo{To be done, now it is only a collection of ideas.}

%Random number generators are tested by taking an output sample. This 

The desired properties of random sequence are \emph{uniformity} (for each bit the probability for both zero and one are exactly $1/2$), \emph{independence} (none of the bits is influenced by any other bit) and \emph{unpredictability} (it is impossible to predict next bit by obtaining any number of previous bits). \cite[p. 1-1]{nist_special}

% RANDOMNESS TESTING - 'theoretical' chapter

\chapter{Randomness testing}

Goal of this chapter is to provide overview of randomness testing process and to explain all used terms. Explanations of both one and two level tests are accompanied by example applications.

\section{Overview / Introduction} \label{chap:rand-intro}

%Testing a sequence (define) (tu01guide - 1, tu01paper - 3, 17), required properties(nistspecial 1-1, 1-3 )

During a randomness test a \emph{random sequence} is tested. In this document, a random sequence is a finite sequence of zero and one bits, which was generated by a tested source of randomness.  \cite[p. 1-1]{nist_special}

%Null/alternative hypothesis (tu01paper - 3, možná učebnice)
%Randomness testing as statistical testing (tu01guide - 2)

Randomness test is a form of \emph{empirical statistical test}, where we test our assumption about the tested data - the \emph{null hypothesis} ($H_0$). During the randomness test it states that the sequence is \emph{random}. Associated with the null-hypothesis is the \emph{alternative hypothesis} ($H_1$), which states that the sequence is \emph{nonrandom}. Goal of the test is to search for evidence against the null-hypothesis. \cite[p. 2]{tu01_guide}

%Idea of testing – extremeness of a sample ? (nistspecial - 1-3+)

%result (accept, reject, Type I/II error) (nistspecial - 1-3+, učebnice 416)

The result of the test is either that we \emph{accept} the null hypothesis (the sequence is considered random), or that we \emph{reject} the null-hypothesis (and accept the alternative hypothesis - the sequence is considered nonrandom). We reject the null hypothesis when the evidence found against the null-hypothesis is strong, otherwise we accept it. Based on the true situation of null hypothesis, four situations depicted in Table \ref{tab:type_errors} may occur. \cite[p. 417]{basic_practice}

\begin{table}
  \begin{tabularx}{0.7\textwidth}{l|c|c}
    %\toprule
    TRUE  & \multicolumn{2}{c}{TEST CONCLUSION}\\
    SITUATION &Accept $H_0$ & Reject $H_0$\\
    \midrule
    $H_0$ is True &  No error & Type I error  \\
    $H_0$ is False & Type II error & No error \\
    %\bottomrule
  \end{tabularx}
  \caption{Possible outcomes when assessing the result of statistical test.}
  \label{tab:type_errors}
\end{table}

\section{Single-level testing} \label{chap:rand-single}
%significance level (tu01 paper 5-8, nistspecial 1-3+, rec for stat testing 5, učebnice)

The \emph{significance level} ($\alpha$) is crucial to assessing the test result and must be set before the test. The $\alpha$ is equal to probability of Type I Error. Usual values are $\alpha = 0.05$ or $\alpha = 0.01$ \cite[p. 390]{basic_practice}, for use in testing of cryptographic random number generators lower values may be chosen. \cite[p. 1-4]{nist_special} The lower $\alpha$ is set, the stronger the found evidence has to be to reject the null hypothesis. %(cite? derived from nistspecial 1-4)

%Test statistic (= function, known distribution), sample (tu01 paper 4+)

The randomness test is defined by a \emph{test statistic} $Y$, which is a function of a finite bit sequence. Distribution of its values under the null hypothesis must be known (or at least approximated). The value of the test statistic ($y$) is computed for the tested random sequence. Each test statistic searches for presence or absence of some "pattern" in the sequence, which would show the nonrandomness of the sequence. There is infinite number of possible test statistics. \cite[p. 4]{tu01_paper}

%p-value (one, two tailed, how extremeness is set) (nist-opt 5, nist-special 1-4,rec for stat testing 5)

The \emph{p-value} of the test is the probability of the test statistic $Y$ taking value at least as extreme as the observed $y$, assuming that the null hypothesis is true. In randomness testing it is equal to the probability that \emph{perfect random generator} would generate less random sequence. The smaller is the p-value, the stronger is the found evidence against the null-hypothesis. \cite[p. 386]{basic_practice} The p-value is calculated based on the observed $y$.



\subsection{Result interpretation} \label{chap:rand-interpretation}
%interpratation (učebnice 380-400)

Decision about the test result is based on the computed \emph{p-value}. If the p-value is lower than the $\alpha$, we \emph{reject the null hypothesis} (and accept the alternative hypothesis), because strong enough evidence against randomness was found. If the p-value is greater than or equal to the $\alpha$, we \emph{accept the null hypothesis}, because the evidence against the null hypothesis was too weak. \cite[p. 390]{basic_practice} It is often recommended to report the \emph{p-value} as well instead of accept/reject only, as it yields more information. \cite[p. 90]{tu01_guide}

% TODO: note on moving alpha, for one alpha fail, for other pass

The p-values close to $\alpha$ can be considered \emph{suspicious}, because they do not clearly indicate rejection. Further testing of the random number generator on \emph{other} random sequences is then in place to search for further evidence. \cite[p. 5]{tu01_paper} The reason is that \emph{randomness} is a probabilistic property, therefore even the perfect random number generator may generate a nonrandom sequence with low p-value (although it is very unlikely). The further evidence is used to differentiate between the bad generator generating a nonrandom sequence systematicaly and the good generator generating nonrandom sequence 'by chance'. \cite[p. 90]{tu01_guide}

%example
\subsection{Example} \label{chap:rand-example}

To demonstrate how a single randomness test is made, the Frequency (Monobit) Test from NIST STS battery was chosen. \cite[p. 2-2]{nist_special} This test is based on testing the fraction of zeroes and ones within the sequence. For a random sequence with length $n$ the  count of ones (and zeroes) is expected to be around $n/2$ (the most probable values are close to $n/2$). 

For the Monobit test it is recommended that the tested sequence has at least 100 bits. The test statistic $S_{obs}$ of the Monobit test is defined as \[S_{obs} = \dfrac{|\#_1 - \#_0|}{\sqrt{n}}\] where $\#_1$ is count of ones in the tested sequence (similarly for zeroes) and $n$ is length of of the tested sequence. Under the null hypothesis, the reference distribution of $S_{obs}$ is half normal (for large $n$). The p-vaue is computed as \[ p = erfc(\dfrac{S_{obs}}{\sqrt{2}}) \] where $erfc$ is the \emph{complementary error function} used to calculate probabilities in normal distribution.

 Let
\[\begin{split}
    \epsilon = 10011001010010000010001001011001101100001101000111\\10101001010010010011100111001100110010010100111011
\end{split}\]
 be the tested sequence. The test statistic for this sequence is 
 \[S_{obs} = \dfrac{|46 - 54|}{\sqrt{100}}\ = \dfrac{|-8|}{10} = 0.8\]
 and the p-value (visualised at Figure \ref{fig:example}) is 
 \[p = erfc\biggl(\dfrac{0.8}{\sqrt{2}}\biggr) \approx 0.423\]

To interpret the test, we compare the computed \emph{p-value} to the chosen $\alpha$. The $\emph{p-value} \approx 0.423$ is greater than both usual $\alpha = 0.05$ and $\alpha = 0.01$, therefore we accept the null hypothesis for both \emph{significance levels} and the sequence $\epsilon$ is considered random.

\begin{figure}
  \begin{center}
    %% minimus is about 100 pixels per 1 centimeter or 300 pixels per 1 inch.
    %% The optimum is about 250 pixels per 1 centimeter 
    \includegraphics[width=8cm]{figures/test_example.png}
  \end{center}
  \caption{Visualization of example p-value for test statistic value $y = 0.8$}
  \label{fig:example}
\end{figure}

% two-level (tu01guide 88, tu01paper 5-8, correcting_dieharder 14)
\section{Two-level testing} \label{chap:rand-two_level}

% two-level motivation (tu01_paper 5-8)
The \emph{two-level test} is done by repeating the single-level test $n$ times. The distribution of produced p-values is then compared to the expected distribution. The two-level test allows the random sequence to be examined both locally and globally, while the single-level test examines the sequence only on the global level. This may lead to discovering local patterns, which cancel out on the global level. \cite[p. 7]{tu01_paper}

To apply the two-level test the tested sequence is split into $n$ equal-length disjoint subsequences. The same single-level test is applied to each of the subsequences (as described in Section \ref{chap:rand-single}) and its \emph{p-values} are collected, resulting in set of $n$ \emph{p-values}\footnote{Note that the p-values are not subject to accept/reject decision.}. The tests are called \emph{first-level tests} and the p-values are called \emph{first-level p-values}. Under the null-hypothesis, the first-level p-values of a given test statistic are uniformly distributed over the interval $(0,1]$. \cite[p. 14]{bad_day} 


% GOF  - 1
The crucial part of two-level test is examining the distribution of \emph{observed first-level p-values}. Usually, the \emph{goodness-of-fit} (GOF) tests are applied as the \emph{second-level test}. \cite[p. 6]{tu01_paper} GOF tests are a family of methods used for examining how well a data sample fits given distribution. \cite[p. 1]{GOF-techniques} The most used GOF tests in randomness testing are the $\chi^2$ (chi-squared) and Kolmogorov-Smirnov test, another notable tests are the Anderson-Darling and Cramér-von-Mises test. \cite[p. 14]{bad_day}

The second-level test is defined by a test statistic $Y$, which is a function of the first-level p-values. Test statistic value ($y$) is calculated from the observed \emph{first-level p-values} and then the \emph{second-level p-value} is calculated from $y$. At lest, the second-level p-value is interpreted as in one-level test (as described in Subsection \ref{chap:rand-interpretation}). 

% ratio of passed sequences
Alternatively, a \emph{proportion of subsequences passing the first-level test} is used to examine the fist-level p-values uniformity. Under the null-hypothesis, it is expected for $n\cdot\alpha$ subsequences to \emph{be rejected} by the first-level test (be a subject to Type I Error). The ratio of sequences passing the first-level test is expected to be around $1-\alpha$, different ratio indicates non-uniformity of observed first-level p-values. \cite[p. 4-2]{nist_special}

% GOF comparison?: (rec_for_stat 6)

% KS - description - (para and non para - 171)
\subsection{Kolmogorov-Smirnov test}
The one-sample Kolmogorov-Smirnov (KS) test is used in randomness testing to compare the observed first-level p-values to the uniform distribution. The Kolmogorov-Smirnov test is built on comparing the cumulative distribution function (CDF)\footnote{For a given distribution and value $x$, the CDF returns the probability of drawing a value less than or equal to $x$.} of the expected distribution and the empirical cumulative distribution function (eCDF)\footnote{For a set of observed data and value $x$, the eCDF returns the probability of drawing a value less than or equal to $x$.} of the observed samples. %cdf - nist_special 1-5 

In first variant, two test statistics are calculated. The test statistic $D^+$ ($D^-$) is the maximal vertical distance between CDF and eCDF above (under) the CDF. In second variant, only the test statistic $D$ (maximal vertical distance between CDF and eCDF) is measured. Formally, the test statistics are defined as
\[\begin{split}
    &D^+ = sup_x\{F_n(x) - F(x)\}\\
    &D^- = sup_x\{F(x) - F_n(x)\}\\
    &D \:\:\:= sup_x\{|F_n(x) - F(x)|\} = max(D^+, D^-)
\end{split}
\] where $F(x)$ is the CDF and $F_n(x)$ is the  eCDF \cite[p. 100]{GOF-techniques}.


\begin{figure}
  \begin{center}
    %% minimus is about 100 pixels per 1 centimeter or 300 pixels per 1 inch.
    %% The optimum is about 250 pixels per 1 centimeter 
    \includegraphics[width=9cm]{figures/ks_d.png}
  \end{center}
  \caption{Visualization of Kolmogorov-Smirnov test statistics for expected uniform distribution and observed data sample.}
  \label{fig:ks_d}
\end{figure}

% $\chi^2$ description (GOF - 100)
\subsection{Chi-squared test}

The Pearson's $\chi^2$ test is used to find statistically significant difference between category frequencies in two categorical sets of data. The first-level p-values are split into $k$ equal-width bins (categories) and their respective frequencies are counted. The counted frequencies are compared to the expected frequencies.

For data with $k$ categories the test statistic $\chi^2$ is defined as \[\chi^2 = \sum_{i=1}^{k} \dfrac{(x_i - m_i)^2}{m_i} \]
where $x_i$ is the observed frequency in $i$-th category and $m_i$ is the expected frequency in $i$-th category. For first-level p-values, the expected frequency is equal in each interval. For a correct test the expected frequency in each category must be at least five. \cite[p. 171]{stat-procedures}

% Two-level example
\subsection{Example}

In the two-level test example, I will test one sequence using both one and two-level testing to demonstrate the difference between them. First, the sequence is tested using the one-level Frequency (Monobit) test from NIST STS battery.\cite[p. 2-2]{nist_special} Then the same sequence is assessed by the two-level test using the Frequency test as the first-level test and KS and $\chi^2$ tests as second-level test. Let
\[\begin{split}
    \epsilon =\:15\: &* (100\:consecutitive\:zeroes) + \\
    15\:&*\:(100\:alternating\:ones\:and\:zeroes) + \\
    5\:&*\:(55\:zeroes\:and\:45\:ones)\:+\:\\
    15\:&*\:(100\:consecutive\:ones)
\end{split}\]
be the tested sequence. 

Result of the one-level Frequency test for the sequence $\epsilon$ is p-value $\approx$ 0.479. The null hypothesis is accepted for both $\alpha = 0.01$ and $\alpha = 0.05$ and the sequence $\epsilon$ is considered random. This sequence however clearly contains a pattern, therefore the probability of it being generated by a perfect random number generator is low.

For the two-level test, the sequence $\epsilon$ is split into $n=50$  disjoint 100 bit long subseqeunces. The Monobit test is applied on each subsequence resulting in set of first-level p-values shown in Table \ref{tab:first_pvalues}.


\begin{table}
  \begin{tabularx}{0.4\textwidth}{ll}
    \toprule
    p-value & occurrences  \\
    \midrule
    $1.52 \cdot 10^{-23}$ & $30$\\
    $0.31$ & $5$\\
    $1.0$ & $15$\\
    \bottomrule
  \end{tabularx}
  \caption{First-level p-values produced by Monobit test}
  \label{tab:first_pvalues}
\end{table}

Next step is to apply the goodness-of-fit tests. The first applied test is the Pearson's $\chi^2$ test with $k=10$ (number of categories), the expected frequency of p-values in each category is five. The statistic of the test is
\[\chi^2 = \sum_{i=1}^{10} \dfrac{(x_i - 5)^2}{5} = 180 \]
and the p-value of this test is $p\approx5.06\cdot10^{-34}$. The null hypothesis is rejected for both $\alpha = 0.01$ and $\alpha = 0.05$. 

Next, I will apply the Kolmogorov-Smirnov test. The eCDF is calculated and then the D statistic is computed. The statistic is $D = 0.6$ and results in p-value $\approx 9.63\cdot10^{-18}$. Again, the null-hypothesis is rejected for both $\alpha = 0.01$ and $\alpha = 0.05$. 

\begin{figure}
  \begin{center}
    %% minimus is about 100 pixels per 1 centimeter or 300 pixels per 1 inch.
    %% The optimum is about 250 pixels per 1 centimeter 
    \includegraphics[width=12.5cm]{figures/two_example.png}
  \end{center}
  \caption{Visualisations for $\chi^2$ and KS tests for two-level test example.}
  \label{fig:two_example}
\end{figure}


%AVAILABLE SOLUTIONS

\chapter{Available solutions}


This chapter serves to describe various works and programs this thesis connects to.


\section{Statistical testing batteries}
More or less deep description of each battery. Should contain information about test parameters/settings.\\
\\
command on how to run battery\\
Mention overflow detection\\
If there are any problems with the battery (e.g. tests which read different amount of data from DieHarder). - Discuss colision with the paper\\
how batteries interpret results (first/second level, more statistics)\\
strong and weak things \\
describe output / image\\
\\

General description of battery - set of tests, choose test by ID, set tested file \\
individual test - smallest executable part, possibly more test statistics, setting of the test(ntup, bitw...), data consumption (fixed vs set by user), number of first levels, return-first level pvalues, second-level p-value,

Randomness testing battery is a set of randomness tests

\emph{Individual test} is the smallest executable unit and applies one two-level test on the tested sequence. Some individual tests apply more than one two-level test. For example, the Diehard Craps Test plays 200~000 games of craps, the test statistics are based on number of wins and number of throws needed to end the game TODO: cite dieharder.

Usually, the following settings are required for each \emph{individual test}.
\begin{markdown*}{%
  hybrid,
  definitionLists,
  footnotes,
  inlineFootnotes,
  hashEnumerators,
  fencedCode,
  citations,
  citationNbsps,
  pipeTables,
  tableCaptions,
}

* number of first-level tests
* test statistic settings
* size of first-level sequence (if applicable)


\end{markdown*}

\subsection{Dieharder}
Explain p-samples, name tests with irregular read bytes
\subsection{NIST STS}
Explain stream-size and stream-count\\
Results - in experiments directory

\subsection{Test U01}
List all batteries.
Explain repetitions, --bit -nb -w -r -s, mention repeating tests with different parameters


\section{Testing toolkits}\label{analysis}
JUST COPIED FROM WORK TO ACADEMIC WRITING COURSE, TO BE USED AND CHANGED LATER.\\

TODO: MENTION INSTALATION



In the previous chapter different randomness testing batteries were described. The typical user, however, uses more than one battery, which means installing and running each testing battery individually. Also it is strongly recommended (sometimes even needed) to set up parameters for each test from the battery individually based on tested file and to run this test manually.

Since this approach is not convenient, Ľubomír Obrátil from Center for Research on Cryptography and Security (CRoCS) at FI MU created the Randomness Testing Toolkit (\emph{RTT}). This toolkit allows users to run and configure three test batteries by a single command.

This work was followed by Patrik Vaverčák from Faculty of Electrical Engineering and Information Technology at Slovak University of Technology. He created newer variant of \emph{RTT} called Randomness Testing Toolkit in Python (\emph{rtt-py}). Compared to \emph{RTT}, it contains two additional test batteries. 

\section{Randomness Testing Toolkit}

\emph{RTT} was created in 2017 and its main idea was to combine \emph{Dieharder}, \emph{NIST STS}\footnote{National Institute of Standards and Technology - Statistical Test Suite} and \emph{Test U01} statistical test batteries into one program. It was written in C++. 

The concept of \emph{RTT} is that it acts only as a unified interface of the batteries. Each test battery is executed by \emph{RTT} as a separate program. The \emph{RTT} then collects the output and processes in into a unified format.~\cite[p.~8]{rtt-obratil}

However some problems in the processing of the output were found; these are addressed in chapter \ref{improvements}.

\subsection{Settings}\label{rtt-settings} 
TODO: MORE RIGID DESCRIPTION
The \emph{RTT} needs to be set up by the user before running. The first part of user settings contains general settings made for the \emph{RTT}, the second part contains configuration for individual test batteries. Each of these parts is stored in its own JSON\footnote{JavaScript Object Notation} file. The original setup description is from

The general settings are stored in \emph{rtt-settings.json} file, which has to be located in the working directory of the \emph{RTT}~\cite[p.~10]{rtt-obratil} . These settings are usually not changed between runs.
The most important setting from the general part are paths to the executable binaries of individual statistical test batteries. This is the only setting that has to be manually filled by the user.

The storage database can also be filled in by the user, but this functionality is often unused. The following general settings have implicit values and do no need to be changed unless the user wishes to. They are paths to storage directories for results and logs of individual runs and execution options (test timeout and maximal number of parallel executions of tests). 

 The battery configurations are dependant on the size of the tested file, therefore the file with the battery configuration is specified for each run of the \emph{RTT}. These configurations are different for each battery (see sections \ref{dieharder}, \ref{nist} and \ref{tu01}), but settings for all of the batteries can be stored together in a single file.~\cite[p.~11]{rtt-obratil}  The \emph{RTT} contains several prepared battery configurations for various sizes of tested file.



\subsection{Output}
The output of \emph{RTT} is in a plain text format. The most important part of the output is the direct report, which is saved in the results directory. At the beginning of the report are general information -- the name of the tested file, the name of the used battery, ratio of passed and failed tests and battery errors and warnings in case there were any.

After the general information is a list of results of individual test runs in a unified format. The fist part of the single test report contains the name of the test and user settings (\emph{e.g. P-sample count in Dieharder battery or Stream size and count in NIST STS battery}). The second part of the single test report are the resulting second-level P-values alongside the names of statistic used (usually Kolmogorov-Smirnov statistic or Chi-Square test). At the end of the single test report is a list of first-level P-values produced by the test. Example of the output can be seen in Figure \ref{fig:rtt_output_example}.

\begin{figure}
  \begin{center}
    %% minimus is about 100 pixels per 1 centimeter or 300 pixels per 1 inch.
    %% The optimum is about 250 pixels per 1 centimeter 
    \includegraphics[width=12cm]{figures/rtt_dieharder_output.png}
  \end{center}
  \caption{The example of single test report from the \emph{RTT}}
  \label{fig:rtt_output_example}
\end{figure}

\subsection{Disadvantages}
The problems/weak points we want to improve with this thesis. Namely at least non machine-machine readable format, running only one battery at time, maybe re-calculation of results

There are two most notable disadvantages of the \emph{RTT}. The first one is that each battery has to be run individually by the user. This lowers the convenience of usage for the user. The second one is the output format. While it is easy to read for human users, machine reading requires complicated parsing. 



\section{Randomness Testing Toolking in Python}
The Randomness Testing Toolkit in Python (\emph{rtt-py}) was created by Patrik Vaverčák. It is supposed to be a better version of \emph{RTT}~\cite[p.~24]{vavercak} and it was written in Python. However there are still some functional differences between \emph{RTT} and \emph{rtt-py}. The most notable difference is in the output format and supported batteries.

\subsection{Settings}
The settings of \emph{rtt-py} are very similar to the original \emph{RTT}. According to Vaverčák, the general settings from the \emph{RTT} should be compatible with \emph{rtt-py}, but in reality there is problem with settings for the NIST STS's experiments directory. Also, no database connection is implemented in \emph{rtt-py}, therefore the \emph{mysql-db} attribute is ignored. \cite{rtt-py-github}


The second part of user settings are tests configurations. They use exactly the same format as those used in \emph{RTT} (as mentioned in \ref{rtt-settings}) and are interchangeable.~\cite[p.~25]{vavercak} The user has to keep in mind that the \emph{rtt-py} uses FIPS\footnote{Federal Information Processing Standards} and BSI\footnote{Bundesamt für Sicherheit in der Informationstechnik} batteries, which are not used in \emph{RTT}.

\subsection{Output}
There is a significant difference in the output format between \emph{RTT} and \emph{rtt-py}. The \emph{rtt-py} creates output in two formats -- \emph{CSV}\footnote{Comma-separated values} and \emph{HTML}\footnote{Hypertext Markup Language}. Both of these report formats contain overview table. Each row from the table represents results of one particular test. The first column contains the name of the test and the name of the battery it belongs to.

The second column contains \emph{failure rate} - ratio representing how many instances of this particular test failed compared to number of executed instances on \emph{all} files with data.


Each of the following columns is named after one tested file. The record contains either P-value reported by the test, or number of failed runs -- this depends on the battery. Example of this table can be seen at figure \ref{fig:rtt_py_table}.
\begin{figure}
  \begin{center}
    %% minimus is about 100 pixels per 1 centimeter or 300 pixels per 1 inch.
    %% The optimum is about 250 pixels per 1 centimeter 
    \frame{\includegraphics[width=14cm]{figures/rtt-py-table2.png}}
  \end{center}
  \caption{The example of the overview table from the \emph{rtt-py}}
  \label{fig:rtt_py_table}
\end{figure}


The output in the HTML format contains more information compared to the output in the CSV format. For each battery and for each tested file an HTML file with reports is generated.


In each report file there is a list of reports for each executed test from the given battery. The single test report contains the result of the test (either reported P-value, or number of failed runs) and it may contain additional information such as settings of the test or other information connected to the result. The contained information depends on the battery and on the executed test. Example of the  report can be seen in figure \ref{fig:rtt_py_html}
\begin{figure}
  \begin{center}
    %% minimus is about 100 pixels per 1 centimeter or 300 pixels per 1 inch.
    %% The optimum is about 250 pixels per 1 centimeter 
    \includegraphics[width=6cm]{figures/rtt-py-dieharder-html.png}
  \end{center}
  \caption{The example of HTML Dieharder report from the \emph{rtt-py}}
  \label{fig:rtt_py_html}
\end{figure}

\subsection{Disdvantages}
One of the problems that need to be addressed is that the \emph{rtt-py } ignores errors and warnings from tests. The most notable example why this is a problem is when the tested file does not contain enough data for current battery configuration.

In this case, the test will read some parts of the data more than once and inform the user about this situation on the error output. The test will still produce result, which will, however, be biased by repeated parts of the tested file.

This may lead to incorrect interpretation of the results and to false acceptance or false rejection of the tested data. Since the \emph{rtt-py} ignores this, there is no way for the user to be informed about this situation.

Compared to the \emph{RTT} the reports created by \emph{rtt-py} contain less information. Namely the first-level P-values are ignored, even though they can be useful for deeper examination of the results and the generator.


% TEST ANALYSIS

\chapter{Tests Analysis}
We can choose from various test statistics. Most of the test statistics in widely used test batteries work with data of fixed length. TODO: REF ANALYSIS CHAPTER However, in some tests data with varying length are tested. These statistics further split into two categories. In the first category, the length of tested data is preset by user. These can be further viewed as fixed-length tests. In the second category, the length of tested data is determined during the testing process. 
\section{Data Consumption}
several big tables, mention exact parameters the tests  were run with

\section{Time Consumption}
again some big tables, choose one test as a reference and the rest will be relative. mention exact parameters, maybe add throughput?
\section{Configuration Calculator}
goal of the config calc, description, usage etc...
\section{P-Values}
Various problems with test p-values distributions, will probably be split into more sections



% IMPLEMENTATIONS COMPARISON
\chapter{Implementations Comparison}

\section{Output}
Mentioned differences\\
for both RTT and rtt-py - subset or whole?

\section{Missing Features of \emph{rtt-py}}

\section{Proposed improvements}
included things: adding first-level p-values, 

% CONCLUSION

\chapter{Conclusion}






\appendix %% Start the appendices.
\chapter{An appendix}
Here you can insert the appendices of your thesis.

\end{document}
